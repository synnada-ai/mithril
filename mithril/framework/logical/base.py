# Copyright 2022 Synnada, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

from collections.abc import KeysView, Mapping
from itertools import chain
from types import UnionType
from typing import Any

from ...utils.utils import OrderedSet
from ..common import (
    NOT_GIVEN,
    TBD,
    AssignedConstraintType,
    BaseKey,
    ConnectionData,
    ConnectionDataType,
    Connections,
    Constraint,
    ConstraintFunctionType,
    ConstraintSolver,
    IOHyperEdge,
    KeyType,
    MainValueInstance,
    MainValueType,
    NullConnection,
    ScalarType,
    ScalarValueType,
    ShapeNode,
    ShapesType,
    ShapeTemplateType,
    ShapeType,
    Tensor,
    ToBeDetermined,
    UniadicRecord,
    Updates,
    UpdateType,
    Variadic,
    create_shape_repr,
    get_shapes,
)
from ..constraints import constraint_type_map

__all__ = ["BaseModel"]


class BaseModel:
    # Disposable models only used once for entire training session.
    # This attribute is only use for manual backends' code generation.

    # TODO: This can be checked from backend's gradient function dict???
    disposable: bool = False
    # TODO: factory_args should be instance variable not class!
    factory_args: dict[str, Any] = {}

    def __init__(
        self,
        name: str | None = None,
        formula_key: str | None = None,
        enforce_jit: bool = True,
    ) -> None:
        self.dag: dict[BaseModel, dict[str, ConnectionData]] = {}
        self._formula_key: str | None = formula_key

        # TODO: maybe set it only to PrimitiveModel / Model.
        self.parent: BaseModel | None = None
        self.assigned_shapes: list[ShapesType] = []
        self.assigned_types: dict[
            str,
            type | UnionType | ScalarType | Tensor[int | float | bool],
        ] = {}
        self.assigned_constraints: list[AssignedConstraintType] = []
        self.conns = Connections()
        self.frozen_attributes: list[str] = []
        self.dependency_map = DependencyMap(self.conns)
        self.name = name
        self._enforce_jit = enforce_jit
        self._jittable = True
        self.constraint_solver: ConstraintSolver = ConstraintSolver()
        self.safe_shapes: dict[str, ShapeTemplateType] = {}
        self.is_frozen = False
        self.inter_key_count = 0

    @property
    def formula_key(self) -> str | None:
        return self._formula_key

    def create_key_name(self) -> str:
        self.inter_key_count += 1
        return "$" + str(self.inter_key_count)

    def _set_outputs(
        self, *args: str | ConnectionData, **kwargs: str | ConnectionData
    ) -> None:
        if self.parent is not None:
            raise Exception("Child model's outputs cannot be set.")
        # Convert all args and kwargs to tuple.
        # Convert all args and kwargs to tuple.
        pairs = tuple([(None, arg) for arg in args]) + tuple(kwargs.items())

        for pair in pairs:
            new_name, name = pair
            metadata = self.conns.extract_metadata(name)

            # Check the connection is valid.
            if (conn_data := self.conns.get_con_by_metadata(metadata)) is None:
                raise KeyError("Requires valid key or Connection to set output!")

            # Check if given metadata is already an output.
            if conn_data in self.conns.output_connections:
                raise KeyError(f"'{conn_data.key}' key is already set as output!")

            if conn_data in self.conns.input_connections:
                raise KeyError("Input of the overall model cannot be set as output.")

            # Autogenerated keys can not be set directly as output without a name.
            if new_name is None and conn_data.key.startswith("$"):
                raise KeyError(
                    "Autogenerated keys can only be set as output if"
                    " a name is provided for the connection as keyworded argument."
                )

            if new_name is not None:  # Non-named connections.
                if new_name in self.conns.all:
                    raise KeyError(f"Key '{new_name}' is already used!")
                self.update_key_name(conn_data, new_name)
            self.conns.set_connection_type(conn_data, KeyType.OUTPUT)
            self.dependency_map.update_globals(OrderedSet({conn_data}))

    def _check_multi_write(
        self,
        local_input: bool,
        local_connection: ConnectionData,
        connection: ConnectionData,
    ) -> None:
        conn_is_output = (
            self.dependency_map.local_output_dependency_map.get(connection, None)
            is not None
        )
        if local_connection.key in self.conns.all and connection.key in self.conns.all:
            local_conn_is_output = (
                self.dependency_map.local_output_dependency_map.get(
                    local_connection, None
                )
                is not None
            )
            if (
                conn_is_output
                and local_conn_is_output
                and local_connection.key != connection.key
            ):
                # Check if 2 connections are part of main model. If it is the case,
                # We expect at least one of them is not an input of the main model,
                # otherwise condition is Multi-write error
                raise Exception(
                    "Given connections are both output connections. Multi-write error!"
                )

        local_val = local_connection.metadata.value
        global_val = connection.metadata.value

        if conn_is_output and not local_input:
            # Check if 2 connections are both output of any models.
            raise Exception(
                "Given connections are both output connections. Multi-write error!"
            )
        elif (
            local_input
            and local_val is not TBD
            # and global_val is not TBD
            and conn_is_output
            and global_val != local_val
        ):
            raise ValueError(
                "An input of the extending model tries to write "
                "to an output connection in the extended model. "
                "Multi-write error!"
            )
        elif not local_input and global_val is not TBD and local_val != global_val:
            raise ValueError(
                "A valued connection of the extended model tries to write "
                "to an output connection of the extending model. "
                "Multi-write error!"
            )

    def _prepare_keys(
        self, model: BaseModel, key: str, connection: ConnectionDataType
    ) -> BaseKey:
        local_connection = model.conns.get_connection(key)
        assert local_connection is not None, "Connection is not found!"
        match connection:
            case NullConnection():
                _connection = BaseKey()
            case str():
                _connection = BaseKey(name=connection)
            case ConnectionData():
                _connection = BaseKey(connections={connection})
            case _ if isinstance(connection, MainValueInstance | Tensor):
                # find_dominant_type returns the dominant type in a container.
                # If a container has a value of type Connection or ExtendTemplate
                # we add necessary models.
                _connection = BaseKey(value=connection)
            case BaseKey():
                expose = connection.expose
                name = connection.name
                # TODO: This check should be removed: conn.connections==set()
                # We should not operate different if _connections is given. Fix this and
                # also fix corresponding tests and dict conversions with "connect".
                if (
                    expose is None
                    and (name is None or self.conns.get_connection(name) is None)
                    and connection.connections == set()
                ):
                    expose = True
                _connection = BaseKey(
                    name=name,
                    expose=expose,
                    connections=connection.connections,
                    type=connection.type,
                    shape=connection.value_shape,
                    value=connection.value,
                )

        return _connection

    def _add_connection(
        self,
        model: BaseModel,
        local_key: str,
        given_connection: BaseKey,
        updates: Updates,
    ) -> tuple[ConnectionData, Updates]:
        is_input = local_key in model.input_keys
        local_connection = model.conns.get_connection(local_key)
        assert local_connection is not None, "Connection is not found!"
        is_not_valued = local_connection.metadata.value is TBD

        d_map = self.dependency_map.local_output_dependency_map
        expose = given_connection.expose
        outer_key = given_connection.name
        con_obj = None
        set_value: (
            ToBeDetermined
            | str
            | ScalarValueType
            | Tensor[int | float | bool]
            | NullConnection
        ) = NOT_GIVEN
        if given_connection.value is not TBD:
            set_value = given_connection.value

        if given_connection.connections == set():
            if outer_key is not None:
                con_obj = self.conns.get_connection(outer_key)
            if outer_key is None or con_obj is None:
                if expose is None and is_input and is_not_valued:
                    expose = True
                con_obj = self._create_connection(local_connection.metadata, outer_key)
            if (
                expose is False
                and is_input
                and set_value is NOT_GIVEN
                and local_connection.metadata.value is TBD
                and (con_obj is None or con_obj not in d_map)
            ):
                raise ValueError(
                    "Expose flag cannot be false when "
                    "no value is provided for input keys!"
                )
        else:
            initial_conn: ConnectionData
            for idx, conn in enumerate(given_connection.connections):
                if isinstance(conn, str):
                    _conn = self.conns.get_connection(conn)
                else:
                    _conn = self.conns.get_con_by_metadata(conn.metadata)
                    if conn in model.conns.all.values():
                        raise ValueError(
                            f"Given connection '{conn.key}' should not "
                            "belong to the extending model!"
                        )

                if not isinstance(_conn, ConnectionData):
                    raise KeyError("Requires accessible connection to be processed!")
                if idx == 0:
                    initial_conn = _conn
                    if outer_key is not None:
                        self.update_key_name(initial_conn, outer_key)
                else:
                    if _conn in d_map:
                        if initial_conn in d_map:
                            raise KeyError(
                                "IOKey object can not have more than one output "
                                "connection. Multi-write error!"
                            )
                        initial_conn, _conn = _conn, initial_conn
                    if (
                        not outer_key
                        and not initial_conn.is_key_autogenerated
                        and not _conn.is_key_autogenerated
                    ):
                        raise KeyError(
                            "Requires a connection to have only one unique key "
                            "name but encountered more!"
                        )
                    updates |= self.merge_connections(initial_conn, _conn)
            if outer_key is None and not initial_conn.is_key_autogenerated:
                outer_key = initial_conn.key
            if not outer_key and initial_conn in d_map and expose is True:
                raise KeyError("Connection without a name cannot be set as output")
            con_obj = initial_conn

        # Name "input" can only be used for input connections.
        is_key_name_input = con_obj is not None and (con_key := con_obj.key) == "input"
        if not is_input and (outer_key == "input" or is_key_name_input):
            raise KeyError(
                "The key 'input' is a reserved key which could not be used for "
                "internal keys."
            )

        if not is_input and not isinstance(set_value, NullConnection):
            raise KeyError(
                f"{local_key} key is an output of the model, output values could "
                "not be set in extend."
            )

        # Inherit submodel connections dict
        self.conns.connections_dict.setdefault(local_connection.metadata, set())
        self.conns.connections_dict[local_connection.metadata] |= (
            model.conns.connections_dict.pop(local_connection.metadata, set())
        )

        # If any value provided, set.
        assert con_obj is not None
        if not isinstance(set_value, NullConnection):
            updates |= con_obj.metadata.set_value(set_value)

        # Check multi-write error for con_obj.
        self._check_multi_write(is_input, local_connection, con_obj)

        # If match required, perform.
        if con_obj.metadata != local_connection.metadata:
            local_key_origin = local_connection.metadata.key_origin
            updates |= self._match_hyper_edges(
                con_obj.metadata, local_connection.metadata
            )
            # If local_connection is an output of the model,
            # update con_obj "key_origin" with local_connection's key_origin.
            if (
                not is_input
                and outer_key not in self.conns.output_keys
                or con_obj.metadata.key_origin is None
            ):
                con_obj.metadata.key_origin = local_key_origin

        unexposed = not (expose or (is_input and con_key in self.conns.io_keys))
        if unexposed:
            if is_input:
                key_type = (KeyType.LATENT_INPUT, KeyType.INTERNAL)[con_obj in d_map]
            else:
                key_type = (KeyType.LATENT_OUTPUT, KeyType.INTERNAL)[con_obj in d_map]
        else:
            key_type = (KeyType.OUTPUT, KeyType.INPUT)[
                is_input and con_obj not in d_map
            ]
        if con_obj in d_map:
            self.conns.couts.discard(con_obj)
        self.conns.set_connection_type(con_obj, key_type)
        # Update Canonicals
        if (
            local_connection in model.conns.cins
            and con_obj in self.conns.input_connections
            and con_obj.metadata.value is TBD
        ):
            self.conns.cins.add(con_obj)

        if local_connection in model.conns.couts and (
            con_obj not in self.dependency_map.local_input_dependency_map
            or con_obj in self.conns.output_connections
        ):
            self.conns.couts.add(con_obj)

        # If any type provided, set using models set_types method
        # in order to execute constraint solver to propagate type
        # updates along the model keys.
        if (set_type := given_connection.type) is not None:
            model._set_types({local_connection: set_type})

        return con_obj, updates

    def update_key_name(self, connection: ConnectionData, key: str) -> None:
        con_data = self.conns.get_extracted_connection(connection)
        key_type = self.conns.get_type(con_data)
        key_dict = self.conns._connection_dict[key_type]
        key_dict[key] = key_dict.pop(con_data.key)
        # Update con_data key
        con_data.key = key
        con_data.is_key_autogenerated = False
        con_data.metadata.key_origin = key

    def merge_connections(
        self, connection1: ConnectionData, connection2: ConnectionData
    ) -> Updates:
        # This method is used if there is 2 Connection objects to represent same Edge.
        # In this case, connection2 is updated with connection1's data and it is removed
        # from dag, dependency_map, self attribute (if exists) and Connections object.

        # TODO: Check multi-write error for Connect type.

        main_connection1 = self.conns.get_con_by_metadata(connection1.metadata)
        main_connection2 = self.conns.get_con_by_metadata(connection2.metadata)

        if (
            main_connection1 is None
            or main_connection2 is None
            or main_connection1 == main_connection2
        ):
            return Updates()

        # Remove main_connection2 from connections dict
        con1_key = main_connection1.key

        if connection2 in self.conns.output_connections:
            if con1_key not in self.conns.output_keys:
                self.conns.set_connection_type(connection1, KeyType.OUTPUT)
            if con1_key in self.input_keys:
                self.conns.set_connection_type(main_connection1, KeyType.INTERNAL)
        elif (
            main_connection2 in self.conns.internal_connections
            and con1_key in self.input_keys
        ):
            self.conns.set_connection_type(main_connection1, KeyType.INTERNAL)

        # Switch all connection2 objects with connection1 object in current dag.
        for m, m_info in self.dag.items():
            local_conns = m.conns.get_cons_by_metadata(main_connection2.metadata)
            if local_conns is None:
                continue

            for local_conn in local_conns:
                if m_info.get(local_conn.key) is not None:
                    self.dag[m][local_conn.key] = main_connection1

        # Update dependecy map, we need to update only local maps
        for (
            o_conn,
            key_info,
        ) in self.dependency_map.local_output_dependency_map.items():
            if main_connection2 in key_info[1]:
                self.dependency_map.local_output_dependency_map[o_conn][1].remove(
                    main_connection2
                )
                self.dependency_map.local_output_dependency_map[o_conn][1].add(
                    main_connection1
                )

        if main_connection2 in self.dependency_map.local_output_dependency_map:
            self.dependency_map.local_output_dependency_map[main_connection1] = (
                self.dependency_map.local_output_dependency_map.pop(main_connection2)
            )

        if main_connection2 in self.dependency_map.local_input_dependency_map:
            old_dependencies = self.dependency_map.local_input_dependency_map.pop(
                main_connection2
            )
            self.dependency_map.local_input_dependency_map.setdefault(
                main_connection1, old_dependencies
            )
            for dependecy in old_dependencies:
                if (
                    dependecy
                    not in self.dependency_map.local_input_dependency_map[
                        main_connection1
                    ]
                ):
                    self.dependency_map.local_input_dependency_map[
                        main_connection1
                    ].append(dependecy)

        self.dependency_map.merge_global_connections(main_connection1, main_connection2)
        self.dependency_map.merge_global_caches(main_connection1, main_connection2)
        updates = self._match_hyper_edges(
            main_connection1.metadata, main_connection2.metadata
        )

        self.conns.remove_connection(main_connection2)

        main_connection2.key = main_connection1.key
        main_connection2.is_key_autogenerated = main_connection1.is_key_autogenerated
        return updates

    def extend(
        self,
        model: BaseModel | BaseModel,
        **kwargs: ConnectionDataType,
    ) -> None:
        # Check possible errors before the extension.
        model.check_extendability()
        if self.parent is not None:
            raise AttributeError("Child model could not be re-extended!")
        if self == model:
            raise KeyError("Model can not extend with itself!")
        if self._enforce_jit and not model.jittable:
            raise Exception(
                "Model with enforced Jit can not be extended by a non-jittable model! \
                            Jit can be unforced by setting enforce_jit = False"
            )
        if model.name is not None:
            # TODO: We could store model names in a set to check if it is unique.
            for m in self.dag:
                if m.name == model.name:
                    raise KeyError(f"Model already has a submodel named {model.name}.")

        model.parent = self
        # Freeze the model.
        model._freeze()

        updates = Updates()

        shape_info: dict[str, ShapeTemplateType] = {}
        type_info: dict[
            str,
            type | UnionType | ScalarType | type[Tensor[int | float | bool]],
        ] = {}

        submodel_dag: dict[str, ConnectionData] = {}
        updates = self.constraint_solver.match(model.constraint_solver)

        # Add canonical output if it is not in external_keys
        external_keys = list(model.external_keys)
        external_keys += [
            item.key for item in model.conns.couts if item.key not in external_keys
        ]

        io_keys: dict[str, BaseKey] = {
            key: self._prepare_keys(model, key, kwargs.get(key, NOT_GIVEN))
            for key in external_keys
        }

        for local_key, value in io_keys.items():
            if value.value_shape is not None:
                shape_info |= {local_key: value.value_shape}

            if value.type is not None:
                type_info[local_key] = value.type

            con_obj, _updates = self._add_connection(model, local_key, value, updates)
            updates |= _updates
            submodel_dag[local_key] = con_obj
            if con_obj.metadata.is_tensor:
                updates.shape_updates.add(con_obj.metadata)

        # Replace shape info keys, which are local keys, with global equivalents.
        shape_info = {
            submodel_dag[key].key: template for key, template in shape_info.items()
        }
        type_info = {
            submodel_dag[key].key: template for key, template in type_info.items()
        }

        # Set given shapes.
        self._set_shapes(
            shape_info,
            updates=updates,
        )  # TODO: Should "trace" be set to True?.

        model.constraint_solver.clear()
        model.conns.connections_dict = {}

        # Insert to self dag as a FrozenDict.""
        # Since we update dag in merge_connections, we could not use FrozenDict.
        self.dag[model] = model_dag = submodel_dag

        self.dependency_map.add_model_dag(model, model_dag)

        # Update jittablity by using model's jittablity.
        self._jittable &= model.jittable

    @staticmethod
    def _update_key_name(
        new_key: str,
        underscored_keys: set[str],
        raw_keys: dict[str, list[str]],
        key_mappings: dict[str, str],
        key_origin: str,
        input_set: set[str],
    ) -> tuple[str, str]:
        # Add underscore if generated key name exists in input keys
        key_prefix = "_"
        # check any of key_prefix + raw_keys[key_origin] in input keys
        flag = True
        while flag:
            flag = False
            for item in raw_keys[key_origin]:
                if key_prefix + key_mappings[item] in input_set | set(
                    key_mappings.values()
                ):
                    key_prefix += "_"
                    flag = True

        new_key = key_prefix + new_key
        underscored_keys.add(key_origin)
        # Update same origin key names that has been previously added.
        for raw_key in raw_keys[key_origin]:
            key_mappings[raw_key] = key_prefix + key_mappings[raw_key]
        raw_keys[key_prefix + key_origin] = raw_keys.pop(key_origin)
        key_origin = key_prefix + key_origin
        return new_key, key_origin

    def generate_keys(
        self,
        symbolic: bool = True,
        include_internals: bool = True,
        include_outputs: bool = False,
    ) -> dict[str, str]:
        if self.dag == {}:
            return {}
        key_mappings: dict[str, str] = {}
        raw_keys: dict[str, list[str]] = {}
        underscored_keys = set[str]()

        if include_outputs:
            input_set = set(self.external_keys)
            keys = "external_keys"
        else:
            input_set = set(self.input_keys)
            keys = "input_keys"

        sorted_inputs = [
            self.dag[m][key].key
            for m in self.get_models_in_topological_order()
            for key in getattr(m, keys)
            if self.dag[m][key].key in input_set
        ]
        # TODO: remove duplicate loop traverse
        for key in sorted_inputs:
            new_key = key
            if key[0] != "$":
                continue
            # TODO: Discuss if we want to generate input key only
            # if len(self.conns.cins) == 1, or we could name all canonical inputs.
            if (
                len(self.conns.cins) == 1
                and key == self._cin.key
                and "input" not in self.input_keys
            ):
                # Handle canonical input
                new_key = "input"
            else:
                key_origin = self.conns.get_key_origin(key)
                assert key_origin is not None
                # Add prefix until key_origin not in underscored_keys and input_keys.
                while (
                    key_origin in (underscored_keys | self.input_keys)
                    or key_origin == "input"
                ):
                    key_origin = "_" + key_origin

                raw_keys.setdefault(key_origin, [])
                key_idx = len(raw_keys[key_origin])
                if key_idx == 0:
                    # Set key origin as is for the initial key.
                    key_suffix = ""
                else:
                    key_suffix = "_" + str(key_idx)
                    if key_idx == 1:
                        # Update initial key if same key origin is encountered
                        # (add index to initial key).
                        raw_key = raw_keys[key_origin][0]
                        key_mappings[raw_key] = key_mappings[raw_key] + "_0"
                        if key_mappings[raw_key] in self.input_keys:
                            new_key, key_origin = self._update_key_name(
                                new_key,
                                underscored_keys,
                                raw_keys,
                                key_mappings,
                                key_origin,
                                set(self.input_keys),
                            )

                new_key = key_origin + key_suffix
                if new_key in self.input_keys:
                    new_key, key_origin = self._update_key_name(
                        new_key,
                        underscored_keys,
                        raw_keys,
                        key_mappings,
                        key_origin,
                        set(self.input_keys),
                    )
                raw_keys[key_origin].append(key)
            key_mappings[key] = new_key

        if include_internals:
            sorted_models = self.get_models_in_topological_order()
            internal_key_mappings: dict[str, str] = {}
            for idx, m in enumerate(sorted_models):
                for key in m.external_keys:
                    outer_conn = self.dag[m][key]
                    outer_key = outer_conn.key
                    if outer_key[0] == "$":
                        # if key is autogenerated, generate a name for the key
                        model_name = m.class_name
                        key_origin = outer_conn.metadata.key_origin
                        assert key_origin is not None

                        generated_name = (
                            "_" + model_name + "_" + str(idx) + "_" + key_origin
                        )

                        # if key is an output key, directly write it
                        # to the internal_key_mappings
                        # or
                        # if key is an input key, first check if the key
                        # is already in internal_key mappings to avoid
                        # overwrite
                        write_to_internal_key_mappings = (
                            key in m.conns.output_keys
                            or internal_key_mappings.get(
                                outer_key, key_mappings.get(outer_key)
                            )
                            is None
                        )

                        while (
                            generated_name in internal_key_mappings.values()
                            and write_to_internal_key_mappings
                        ):
                            assert key_origin is not None
                            key_origin = "_" + key_origin
                            generated_name = (
                                "_" + model_name + "_" + str(idx) + "_" + key_origin
                            )

                        if write_to_internal_key_mappings:
                            internal_key_mappings[outer_key] = generated_name

            key_mappings = internal_key_mappings | key_mappings
        if symbolic:
            key_mappings = {key: "$" + value for key, value in key_mappings.items()}
        return key_mappings

    def get_unique_submodel_names(self) -> dict[BaseModel, str]:
        name_mapping: dict[BaseModel, str] = {}
        existing_names: set[str] = set()
        model_type_dict: dict[str, list[BaseModel]] = {}

        # First, assign existing names and track used names.
        # Also save unnamed models to model_type_dict.
        for model in self.dag:
            if model.name:
                name_mapping[model] = model.name
                existing_names.add(model.name)
            else:
                model_type_dict.setdefault(model.class_name, []).append(model)

        # Iterate over different model types among unnamed models.
        for model_type, model_list in model_type_dict.items():
            counter = 0
            # Iterate over same class model objects to name them.
            for i, model in enumerate(model_list):
                if len(model_list) == 1:
                    # If there is only one model of a type, do not increment counter.
                    counter -= 1
                    name = model_type
                else:
                    name = f"{model_type}_{counter + i}"
                while name in existing_names:
                    counter += 1  # counter is incremented until a unique name is found.
                    name = f"{model_type}_{counter + i}"
                name_mapping[model] = name
                existing_names.add(name)
        return name_mapping

    def _freeze(self) -> None:
        for cout in self.conns.couts:
            self.conns.set_connection_type(cout, KeyType.OUTPUT, safe=False)
        self.dependency_map.update_all_keys()

        # Name unnamed submodels before freezing considering the insertion order.
        model_names = self.get_unique_submodel_names()
        for m in self.dag:
            if m.name is None:
                m.name = model_names[m]

        if self.formula_key is not None:
            # Must be convertable to primitive.
            assert len(self.conns.output_keys) == 1, (
                "Logical models have altenative primitive implementation must "
                "have only 1 output."
            )
        # super()._freeze()
        self.is_frozen = True

    @staticmethod
    def _reverse_dfs(
        node: BaseModel,
        graph: dict[BaseModel, OrderedSet[BaseModel]],
        top_order: list[BaseModel],
        visited: set[BaseModel],
    ) -> None:
        visited.add(node)
        for m in graph[node]:
            if m not in visited:
                BaseModel._reverse_dfs(m, graph, top_order, visited)
        top_order.append(node)

    def get_models_in_topological_order(self) -> list[BaseModel]:
        dependency_map = self.dependency_map.local_output_dependency_map
        graph = {
            info[0]: OrderedSet(
                [dependency_map[spec][0] for spec in info[1] if spec in dependency_map]
            )
            for info in dependency_map.values()
        }
        top_order: list[BaseModel] = list()
        visited: set[BaseModel] = set()
        for model in graph:
            if model not in top_order:
                BaseModel._reverse_dfs(model, graph, top_order, visited)
        return top_order

    # TODO: Summary should be isolated from the model.
    def extract_connection_info(
        self,
        name_mappings: dict[BaseModel, str],
        data_to_key_map: dict[IOHyperEdge, list[str]] | None = None,
        data_memo: Mapping[int, IOHyperEdge] | None = None,
    ) -> dict[str, tuple[dict[str, list[str]], dict[str, list[str]]]]:
        conn_info: dict[str, tuple[dict[str, list[str]], dict[str, list[str]]]] = {}
        if self.input_keys:
            if data_to_key_map is None:
                data_to_key_map = {}
            if data_memo is None:
                data_memo = {}
            model_key_map: dict[BaseModel, dict[str, str]] = {}

            # handle the case when model is constructed with += operation. In that case,
            # directly take canonical output as the output_key.

            # TODO: We may expose all canonical outputs for summary instead of
            # checking only len(self.conns.couts) == 1.
            output_keys = (
                ([self._cout.key] if len(self.conns.couts) == 1 else [])
                if not self.conns.output_keys
                else self.conns.output_keys
            )
            # extract key mappings and data map of outer model
            key_mappings = self.generate_keys(
                include_internals=False, include_outputs=True
            )
            data_map = {key: conn.metadata for key, conn in self.conns.all.items()}

            # Sort in topological order
            sorted_models = self.get_models_in_topological_order()

            for model in sorted_models:
                model_name = name_mappings[model]
                m_info = self.dag[model]
                # set default structure of conn_info and shape_info
                conns = conn_info.setdefault(model_name, ({}, {}))
                # include input keys with Tensor value
                input_keys = tuple(model.input_keys)
                # Generate sub_model key_map and data map
                model_key_map[model] = m_key_mappings = model.generate_keys(
                    include_internals=False, include_outputs=True
                )
                m_data_map = {
                    key: conn.metadata for key, conn in model.conns.all.items()
                }
                for inner_key in input_keys + tuple(model.conns.output_keys):
                    # Find the data of the key, if data memo is given, extract its
                    # copied version and extract the shapes
                    key_data = data_memo.get(
                        id(m_data_map[inner_key]), m_data_map[inner_key]
                    )

                    # Find inner and outer keys. Also find their updated version based
                    # on their key mappings
                    updated_inner_key = m_key_mappings.get(inner_key, inner_key)
                    outer_conn = m_info[inner_key]
                    outer_key = outer_conn.key
                    updated_outer_key = data_to_key_map.get(
                        key_data, [key_mappings.get(outer_key, outer_key)]
                    )

                    # take and setdefault connection list in which update will be done
                    conn = conns[inner_key in model.conns.output_keys].setdefault(
                        updated_inner_key, []
                    )
                    if inner_key not in input_keys:
                        continue

                    if (val := key_data.value) is not TBD:
                        conn.append(str(val))

                    elif outer_key in self.input_keys:
                        # If outer_key in input_keys of overall model, it means
                        # the input key is overall input to the model. Do the
                        # updates accordingly
                        input_name = ["'" + key + "'" for key in updated_outer_key]
                        conn.extend(input_name)
                    else:
                        # if input_key is not in self.input_keys, that means this
                        # input key connected to a model and it is an internal
                        # connection. Find the connected model and do the intializations
                        con_model = self.dependency_map.local_output_dependency_map[
                            outer_conn
                        ][0]
                        con_generated_keys = model_key_map.setdefault(
                            con_model,
                            con_model.generate_keys(
                                include_internals=False, include_outputs=True
                            ),
                        )
                        conn_info.setdefault(name_mappings[con_model], ({}, {}))
                        model_conn = m_info[inner_key]
                        con = con_model.conns.get_con_by_metadata(model_conn.metadata)
                        assert con is not None, "Connection is not found"
                        con_key = con.key

                        con_key = con_generated_keys.get(con_key, con_key)
                        # Since being internal key means having two sided connection,
                        # Two updates on conn_info dict needs to be done. one for
                        # model's input key and other for connected_model's output
                        # key. do the updates accordingly.
                        conn_info[model_name][0].setdefault(
                            updated_inner_key, []
                        ).append(name_mappings[con_model] + "." + con_key)
                        conn_info[name_mappings[con_model]][1].setdefault(
                            con_key, []
                        ).append(model_name + "." + updated_inner_key)

            for outer_key in output_keys:
                # Lastly, traverse through output keys of the overall model
                # Find the connected model, and find the inner key by finding
                # the metadata
                metadata = self.conns.get_metadata(outer_key)
                outer_out_conn = self.conns.get_connection(outer_key)

                assert metadata is not None, "Metadata is not found!"
                assert outer_out_conn is not None, "Connection is not found"

                model = self.dependency_map.local_output_dependency_map[outer_out_conn][
                    0
                ]
                other_conn = model.conns.get_con_by_metadata(metadata)
                assert other_conn is not None, "Connection is not found"

                inner_key = other_conn.key
                updated_inner_key = model_key_map[model].get(inner_key, inner_key)
                key_data = data_memo.get(id(data_map[outer_key]), data_map[outer_key])
                updated_outer_key = data_to_key_map.get(
                    key_data, [key_mappings.get(outer_key, outer_key)]
                )
                if updated_outer_key[0][0] == "$":
                    # There is only possibilty of outer key is found to be with $ sign.
                    # That is, if model is constructed with += operator. In that case,
                    # canonical output will be external key even if it is not named by
                    #  user. Therefore, handle the case with dicrectly writing $output
                    updated_outer_key = ["$output"]
                model_name = name_mappings[model]
                conn_info[model_name][1][updated_inner_key].extend(
                    ["'" + key + "'" for key in updated_outer_key]
                )

        return conn_info

    @property
    def grad_formula(self) -> str:
        if self.formula_key is None:
            raise AttributeError("Model has no formula key!")
        return self.formula_key + "_grad"

    @property
    def class_name(self) -> str:
        return self.__class__.__name__

    @property
    def enforce_jit(self) -> bool:
        return self._enforce_jit

    @enforce_jit.setter
    def enforce_jit(self, value: bool) -> None:
        self._enforce_jit = value

    @property
    def jittable(self) -> bool:
        return self._jittable

    @property
    def shapes(
        self,
    ) -> Mapping[str, ShapeTemplateType | list[ShapeTemplateType] | None]:
        return self.get_shapes()

    @property
    def external_keys(self) -> KeysView[str]:
        return self.conns.io_keys

    @property
    def input_keys(self) -> KeysView[str]:
        return self.conns.input_keys

    @property
    def _all_keys(self) -> KeysView[str]:
        return self.conns.all.keys()

    @property
    def output_keys(self) -> list[str]:
        output_keys = list(self.conns.output_keys)
        return output_keys

    def check_extendability(self) -> None:
        # Check possible errors before the extension.
        if self.parent is not None:
            raise AttributeError("Submodel of a model could not be extended!")

    def _get_outermost_parent(self) -> BaseModel:
        model = self
        while model.parent is not None:
            model = model.parent
        return model

    def __setattr__(self, name: str, value: Any) -> None:
        # You need to be careful here to avoid infinite recursion
        if (
            getattr(self, "frozen_attributes", None) is not None
            and name in self.frozen_attributes
        ):
            # To avoid infinite recursion, use the base class's __setattr__ method
            raise AttributeError(
                f"{name} attribute of {self.__class__.__name__} type object is frozen."
            )
        else:
            super().__setattr__(name, value)

    def _create_key_name(self) -> str:
        self.inter_key_count += 1
        return "$" + str(self.inter_key_count)

    # TODO: Rename it to _create_connection_data
    def _create_connection(
        self, metadata: IOHyperEdge, key: str | None = None
    ) -> ConnectionData:
        # If key is not provided, create a new key name and
        # label it as auto-generated.
        if key is not None and self.conns.get_connection(key) is not None:
            raise KeyError("Connection with name " + key + " already exists!")
        if is_key_autogenerated := key is None:
            key = self._create_key_name()

        con = ConnectionData(
            key=key, metadata=metadata, is_key_autogenerated=is_key_autogenerated
        )
        if not is_key_autogenerated:
            # Set key_origin into metadata
            metadata.key_origin = key

        self.conns.add(con)
        return con

    def _set_shapes(
        self,
        shapes: ShapesType,
        trace: bool = False,
        updates: Updates | None = None,
        **kwargs: ShapeTemplateType,
    ) -> None:
        # Initialize assigned shapes dictionary to store assigned shapes.
        assigned_shapes: dict[str, ShapeTemplateType] = {}

        if updates is None:
            updates = Updates()

        model = self._get_outermost_parent()
        used_keys: dict[str | int, ShapeType] = {}
        shape_nodes: dict[str | ConnectionData, tuple[ShapeNode, str]] = {}
        # TODO: Can this be refactored to use a single loop?
        for key, shape in chain(shapes.items(), kwargs.items()):
            metadata = self.conns.extract_metadata(key)
            given_repr = create_shape_repr(shape, model.constraint_solver, used_keys)
            # GetÂ inner string representation of the metadata and save
            # use this name in order to merge .
            conn = self.conns.get_con_by_metadata(metadata)
            assert conn is not None
            inner_key = conn.key
            shape_nodes[key] = (given_repr.node, inner_key)
            assigned_shapes[inner_key] = shape
        # Apply updates to the shape nodes.
        for key in chain(shapes, kwargs):
            node, _inner_key = shape_nodes[key]
            if (metadata := self.conns.get_data(_inner_key)).is_polymorphic:
                # If edge_type is not defined yet, set it to Tensor since
                # shape is provided.
                updates |= metadata.set_type(Tensor[int | float | bool])
            shape_node = self.conns.get_shape_node(_inner_key)
            assert shape_node is not None
            updates |= shape_node.merge(node)

        if trace:
            self.assigned_shapes.append(assigned_shapes)

        model.constraint_solver(updates)

    def _set_types(
        self,
        config: Mapping[
            str | ConnectionData,
            type | UnionType | ScalarType | type[Tensor[int | float | bool]],
        ]
        | Mapping[
            ConnectionData,
            type | UnionType | ScalarType | type[Tensor[int | float | bool]],
        ]
        | Mapping[
            str,
            type | UnionType | ScalarType | type[Tensor[int | float | bool]],
        ]
        | None = None,
        **kwargs: type | UnionType | ScalarType | type[Tensor[int | float | bool]],
    ) -> None:  # Initialize assigned shapes dictionary to store assigned shapes.
        if config is None:
            config = {}

        assigned_types: dict[
            str,
            type | UnionType | ScalarType | Tensor[int | float | bool],
        ] = {}

        # Get the outermost parent as all the updates will happen here.
        model = self._get_outermost_parent()
        updates = Updates()
        for key, key_type in chain(config.items(), kwargs.items()):
            metadata = self.conns.extract_metadata(key)
            conn = self.conns.get_con_by_metadata(metadata)
            assert conn is not None
            inner_key = conn.key
            assigned_types[inner_key] = key_type
            updates |= metadata.set_type(key_type)
        # Store assigned types in the model.
        self.assigned_types |= assigned_types
        # Run the constraints for updating affected connections.
        model.constraint_solver(updates)

    def _set_value(
        self,
        key: ConnectionData,
        value: MainValueType | Tensor[int | float | bool] | str,
    ) -> Updates:
        """
        Set value for the given connection.

        Args:
            key (str | Connection): Connection key or Connection object to set value.
            value (MainValueType): Value to set for the given connection.

        Raises:
            KeyError: If the provided key is not a valid IO key.
        """

        if key.key not in self.conns.input_keys:
            raise ValueError("Values of internal and output keys cannot be set.")
        if value != TBD:
            self.conns.cins.discard(key)
        # Data is scalar, set the value directly.
        return key.metadata.set_value(value)

    def _set_values(
        self,
        config: Mapping[
            str | ConnectionData, Tensor[int | float | bool] | MainValueType | str
        ]
        | Mapping[ConnectionData, Tensor[int | float | bool] | MainValueType | str]
        | Mapping[str, Tensor[int | float | bool] | MainValueType | str]
        | None = None,
        **kwargs: Tensor[int | float | bool] | MainValueType | str,
    ) -> None:
        """
        Set multiple values in the model.
        This method updates values in the outermost model by traversing up the
        parent hierarchy. It performs metadata extraction on self, validity checks
        and updates on the parent model. Finally, it solves constraints
        with the updated values.

        Args:
            config (dict[str | Connection, MainValueType]): A dictionary where
            keys are either strings or Connection objects, and values are
            of type MainValueType.
            **kwargs (MainValueType): Key-value pairs where keys are string names
            of connections present in this model.

        Raises:
            KeyError: If a valid key or Connection is not provided in the values
            dictionary.
        """
        if config is None:
            config = {}
        # Make all value updates in the outermost model.s
        model = self._get_outermost_parent()
        updates = Updates()
        for key, value in chain(config.items(), kwargs.items()):
            # Perform metadata extraction process on self.
            metadata = self.conns.extract_metadata(key)
            # Perform validity check and updates on model.
            if (conn_data := model.conns.get_con_by_metadata(metadata)) is None:
                raise KeyError("Requires valid key or Connection to set values!")
            updates |= model._set_value(conn_data, value)

        # Solve constraints with the updated values.
        model.constraint_solver(updates)

    def get_shapes(
        self,
        uni_keys: dict[UniadicRecord, str] | None = None,
        var_keys: dict[Variadic, str] | None = None,
        symbolic: bool = True,
        verbose: bool = False,
    ) -> Mapping[str, ShapeTemplateType | list[ShapeTemplateType] | None]:
        return get_shapes(
            data_dict={key: value.metadata for key, value in self.conns.all.items()},
            uniadic_keys=uni_keys,
            varadic_keys=var_keys,
            symbolic=symbolic,
            verbose=verbose,
            key_mappings=self.generate_keys(include_outputs=True),
        )

    def _add_constraint(
        self,
        fn: ConstraintFunctionType,
        keys: list[str],
        types: list[UpdateType] | None = None,
        dependencies: set[Constraint] | None = None,
    ) -> Constraint:
        all_conns = self.conns.all
        hyper_edges = [all_conns[key].metadata for key in keys]

        if dependencies is None:
            dependencies = set()
        unresolved_dependencies = (
            dependencies & self.constraint_solver.constraint_map.keys()
        )
        if types is None:
            types = constraint_type_map.get(fn, [UpdateType.SHAPE, UpdateType.VALUE])
        constr = Constraint(fn=fn, types=types)
        constr.add_dependencies(*unresolved_dependencies)

        self.constraint_solver.constraint_map[constr] = hyper_edges
        for hyper_edge in hyper_edges:
            hyper_edge.add_constraint(constr)

        self.constraint_solver.solver_loop({constr})
        return constr

    def add_constraint(
        self,
        fn: ConstraintFunctionType,
        keys: list[str],
        type: list[UpdateType] | None = None,
        dependencies: set[Constraint] | None = None,
    ) -> Constraint:
        self.assigned_constraints.append({"fn": fn.__name__, "keys": keys})
        return self._add_constraint(fn, keys, type, dependencies)

    @property
    def _cin(self) -> ConnectionData:
        if (cin_len := len(self.conns.cins)) != 1:
            raise KeyError(
                f"Currently, there exists {cin_len} canonical inputs, model "
                "should have exactly one canonical input!"
            )
        return next(iter(self.conns.cins))

    @property
    def _cout(self) -> ConnectionData:
        if (cout_len := len(self.conns.couts)) != 1:
            raise KeyError(
                f"Currently, there exists {cout_len} canonical outputs, model "
                "should have exactly one canonical output!"
            )

        return next(iter(self.conns.couts))

    def _set_cin(self, *connections: str | ConnectionData, safe: bool = True) -> None:
        self.conns.cins = set()
        for given_conn in connections:
            conn = self.conns.get_extracted_connection(given_conn)

            is_valued = conn.metadata.value is not TBD
            if conn not in self.dependency_map.local_input_dependency_map:
                raise ValueError(
                    "To set a connection as canonical input, connection must be an "
                    "input connection!"
                )
            elif is_valued:
                if safe:
                    raise ValueError(
                        "To set a connection as canonical input, "
                        "connection must be unvalued!"
                    )
            else:
                self.conns.cins.add(conn)

    def _set_cout(self, *connections: str | ConnectionData, safe: bool = True) -> None:
        self.conns.couts = set()
        for given_conn in connections:
            conn = self.conns.get_extracted_connection(given_conn)
            is_valued = conn.metadata.value is not TBD
            if conn not in self.dependency_map.local_output_dependency_map or is_valued:
                if safe:
                    raise ValueError(
                        "To set a connection as canonical output, "
                        "connection must be an output connection!"
                    )
            else:
                self.conns.couts.add(conn)

    def _match_hyper_edges(self, left: IOHyperEdge, right: IOHyperEdge) -> Updates:
        # if type(left.data) is not type(right.data):
        l_type = left.edge_type
        r_type = right.edge_type
        if ((l_type is Tensor) ^ (r_type is Tensor)) and (
            ToBeDetermined not in (l_type, r_type)
        ):
            raise TypeError(
                "Types of connections are not consistent. Check connection types!"
            )

        right_connections = self.conns.connections_dict.pop(right, set())
        self.conns.connections_dict[left] |= right_connections

        for conns_obj in right_connections:
            conns = conns_obj.metadata_dict.pop(right, None)

            if conns is None:
                raise KeyError(
                    "Requires merged connection's metadata to contain its "
                    "Connections object."
                )

            if left not in conns_obj.metadata_dict:
                conns_obj.metadata_dict[left] = conns
            elif left in conns_obj.metadata_dict:
                conns_obj.metadata_dict[left] |= conns

            for conn in conns:
                conn.metadata = left

        # Update IOHyperEdge's in constraint solver.
        self.constraint_solver.update_constraint_map(left, right)
        # Match data of each IOHyperEdge's.
        updates = left.match(right)
        return updates


class DependencyMap:
    """
    Depedency Map stores relations between connections and models
    TODO: Write doc
    """

    def __init__(self, connections: Connections) -> None:
        self.conns = connections
        # Stores relation between input_keys to dependent output connections
        self._global_input_dependency_map: dict[
            ConnectionData, OrderedSet[ConnectionData]
        ] = {}
        # Stores relation between output_keys to dependent input connections
        self._global_output_dependency_map: dict[
            ConnectionData, OrderedSet[ConnectionData]
        ] = {}

        # Caches relations during extend to avoid traverse whole graph
        self._global_input_dependency_map_cache: dict[
            ConnectionData, OrderedSet[ConnectionData]
        ] = {}
        self._global_output_dependency_map_cache: dict[
            ConnectionData, OrderedSet[ConnectionData]
        ] = {}
        # Stores releation between local input keys to dependent local
        # output connections
        self.local_input_dependency_map: dict[
            ConnectionData, list[tuple[BaseModel, OrderedSet[ConnectionData]]]
        ] = {}
        # Stores releation between local output keys to dependent local
        # input connections
        self.local_output_dependency_map: dict[
            ConnectionData, tuple[BaseModel, OrderedSet[ConnectionData]]
        ] = {}

    # Add new model to dependency map, model_dag is created in extend
    def add_model_dag(
        self, model: BaseModel, model_dag: dict[str, ConnectionData]
    ) -> None:
        updated_conns: OrderedSet[ConnectionData] = OrderedSet()
        for local_key, conn in model_dag.items():
            if local_key in model.conns.input_keys:
                specs: OrderedSet[ConnectionData] = OrderedSet(
                    [
                        model_dag[conn.key]
                        for conn in model.dependency_map.get_dependent_output_conns(
                            local_key
                        )
                        if model_dag.get(conn.key) is not None
                    ]
                )
                self.local_input_dependency_map.setdefault(conn, []).append(
                    (model, specs)
                )
                updated_conns.add(conn)
            else:
                specs = OrderedSet(
                    [
                        model_dag[conn.key]
                        for conn in model.dependency_map.get_dependent_input_conns(
                            local_key
                        )
                        if model_dag.get(conn.key) is not None
                    ]
                )
                self.local_output_dependency_map[conn] = (model, specs)

                updated_conns.add(conn)
                self.cache_internal_references(conn, specs)

            if self.look_for_cyclic_connection(conn, specs):
                raise KeyError(
                    f"There exists a cyclic subgraph between {conn.key} key and "
                    f"{[spec.key for spec in specs]} key(s)!"
                )

        self.update_globals(updated_conns)

    # Caches extended connections to avoid traverse
    def cache_internal_references(
        self, output_conn: ConnectionData, dependent_conns: OrderedSet[ConnectionData]
    ) -> None:
        # Be sure all input and output keys has cache entry
        for conn in self.conns.input_connections:
            self._global_input_dependency_map_cache.setdefault(conn, OrderedSet())

        for conn in self.conns.output_connections:
            self._global_output_dependency_map_cache.setdefault(conn, OrderedSet())

        all_dependent_input_conns: OrderedSet[ConnectionData] = OrderedSet()

        for input_conn in dependent_conns:
            # Extend from output, update global input dependency map with new
            # output conn
            for dependent_conn in self._global_output_dependency_map_cache.get(
                input_conn, OrderedSet()
            ):
                all_dependent_input_conns.add(dependent_conn)

                # If not extend from input add
                if output_conn not in self._global_input_dependency_map_cache:
                    self._global_input_dependency_map_cache[dependent_conn].add(
                        output_conn
                    )

            # Input is not internal
            if input_conn in self.conns.input_connections:
                all_dependent_input_conns.add(input_conn)

                # If not extend from input add
                if output_conn not in self._global_input_dependency_map_cache:
                    self._global_input_dependency_map_cache[input_conn].add(output_conn)

        # Extend from Input
        if output_conn in self._global_input_dependency_map_cache:
            dependent_output_conns = self._global_input_dependency_map_cache.pop(
                output_conn
            )

            # Update global_input_dependencies
            for dependent_input_conn in all_dependent_input_conns:
                if dependent_input_conn in self._global_input_dependency_map_cache:
                    self._global_input_dependency_map_cache[
                        dependent_input_conn
                    ].discard(output_conn)
                    self._global_input_dependency_map_cache[dependent_input_conn] |= (
                        dependent_output_conns
                    )

            # Update global_output_dependencies
            for dependent_output_conn in dependent_output_conns:
                if dependent_output_conn in self._global_output_dependency_map_cache:
                    self._global_output_dependency_map_cache[
                        dependent_output_conn
                    ].discard(output_conn)
                    self._global_output_dependency_map_cache[dependent_output_conn] |= (
                        all_dependent_input_conns
                    )

        else:
            self._global_output_dependency_map_cache.setdefault(
                output_conn, OrderedSet()
            )
            self._global_output_dependency_map_cache[output_conn] |= (
                all_dependent_input_conns
            )

    # Caches given input connection for later usage
    def cache_conn_input_dependency(self, conn: ConnectionData) -> None:
        if conn not in self._global_input_dependency_map_cache:
            dependents = self.get_output_key_dependency(conn.key)
            self._global_input_dependency_map_cache[conn] = dependents

    # Caches given output connection for later usage
    def cache_conn_output_dependency(self, conn: ConnectionData) -> None:
        if conn not in self._global_output_dependency_map_cache:
            dependents = self.get_input_key_dependency(conn.key)
            self._global_output_dependency_map_cache[conn] = dependents

    # Returns dependent input keys of given output key
    def get_dependent_input_conns(self, key: str) -> OrderedSet[ConnectionData]:
        if (given_conn := self.conns.get_connection(key)) is None:
            raise KeyError("Given key does not belong to the Model!")
        dependent_conns: OrderedSet[ConnectionData] = OrderedSet()
        if key in self.conns.output_keys:
            dependent_conns = self._global_output_dependency_map[given_conn]
        elif (
            conn := self.conns.get_connection(key)
        ) in self._global_output_dependency_map_cache:
            return self._global_output_dependency_map_cache[conn]
        else:
            return self.get_output_key_dependency(key)

        return dependent_conns

    # Return dependent output keys of given input key
    def get_dependent_output_conns(self, key: str) -> OrderedSet[ConnectionData]:
        if (given_conn := self.conns.get_connection(key)) is None:
            raise KeyError("Given key does not belong to the Model!")
        dependent_conns: OrderedSet[ConnectionData] = OrderedSet()
        if key in self.conns.input_keys:
            dependent_conns = self._global_input_dependency_map[given_conn]
        elif (
            conn := self.conns.get_connection(key)
        ) in self._global_input_dependency_map_cache:
            return self._global_input_dependency_map_cache[conn]
        else:
            return self.get_input_key_dependency(key)

        return dependent_conns

    # Update dependecy map
    def update_all_keys(self) -> None:
        # This method is used in freeze, because in freeze dependencies changed
        # without updating dependency map.
        self.update_globals(
            OrderedSet(self.conns.input_connections)
            | OrderedSet(self.conns.output_connections)
        )

    # Get dependent output connections if given input connection is cached
    # else returns None
    def _get_from_input_cache(self, conn: ConnectionData) -> OrderedSet[ConnectionData]:
        dependent_conns = self._global_input_dependency_map_cache.get(
            conn, OrderedSet()
        )
        dependent_conns = OrderedSet(
            [
                dependent_conn
                for dependent_conn in dependent_conns
                if dependent_conn.key in self.conns.output_keys
            ]
        )
        return dependent_conns

    # Get dependent input connections if given output connection is cached
    # else returns None
    def _get_from_output_cache(
        self, conn: ConnectionData
    ) -> OrderedSet[ConnectionData]:
        dependent_conns = self._global_output_dependency_map_cache.get(
            conn, OrderedSet()
        )
        dependent_conns = OrderedSet(
            [
                dependent_conn
                for dependent_conn in dependent_conns
                if dependent_conn.key in self.conns.input_keys
            ]
        )
        return dependent_conns

    # Update global dependency maps wrt given connections
    def update_globals(self, updated_conns: OrderedSet[ConnectionData]) -> None:
        for input_conn in self.conns.input_connections:
            self._global_input_dependency_map.setdefault(input_conn, OrderedSet())

        for output_conn in self.conns.output_connections:
            self._global_output_dependency_map.setdefault(output_conn, OrderedSet())

        visited_keys: OrderedSet[ConnectionData] = OrderedSet()
        while updated_conns:
            conn = updated_conns.pop()
            if conn in visited_keys:
                continue

            visited_keys.add(conn)
            dependent_conns: OrderedSet[ConnectionData] = OrderedSet()

            if conn.key in self.conns.input_keys:
                # New global input key
                dependent_conns = self._get_from_input_cache(conn)

                for dependent_conn in dependent_conns:
                    self._global_input_dependency_map[conn].add(dependent_conn)

            elif conn.key in self.conns.output_keys:
                # New global output key
                dependent_conns = self._get_from_output_cache(conn)

                for dependent_conn in dependent_conns:
                    self._global_output_dependency_map[conn].add(dependent_conn)

            else:
                # Key must be overriden, remove from dependecy map
                if conn in self._global_input_dependency_map:
                    dependent_conns = self._global_input_dependency_map.pop(
                        conn, OrderedSet()
                    )
                    for dependent_conn in dependent_conns:
                        self._global_output_dependency_map[dependent_conn].remove(conn)

                dependent_conns = OrderedSet()
            updated_conns |= OrderedSet(dependent_conns)

    # Retrieve dependent output connection keys given input key by traversing the graph.
    def get_input_key_dependency(self, key: str) -> OrderedSet[ConnectionData]:
        if (given_conn := self.conns.get_connection(key)) is None:
            raise KeyError("Given key does not belong to the Model!")
        # If there already exists any input keys, add them.
        specs = OrderedSet(
            [
                key
                for item in self.local_input_dependency_map[given_conn]
                for key in item[1]
                if key in self.conns.output_keys
            ]
        )

        # Make search from intermediate keys to the input keys.
        key_stack = OrderedSet(
            [
                spec
                for item in self.local_input_dependency_map[given_conn]
                for spec in item[1]
                if spec not in specs
            ]
        )
        while key_stack:
            conn_data = key_stack.pop()
            if conn_data.key in self.conns.output_keys:
                specs.add(conn_data)
            # key_stack.update(self.dependency_map.get(key.key, OrderedSet()))
            # TODO: add test checking the while
            key_stack |= (
                OrderedSet(
                    [
                        spec
                        for item in self.local_input_dependency_map[conn_data]
                        for spec in item[1]
                    ]
                )
                if conn_data in self.local_input_dependency_map
                else OrderedSet()
            )
        return specs

    # Retrieve dependent input connection keys given output key by traversing the graph.
    def get_output_key_dependency(self, key: str) -> OrderedSet[ConnectionData]:
        if (given_conn := self.conns.get_connection(key)) is None:
            raise KeyError("Given key does not belong to the Model!")

        # If there already exists any input keys, add them
        specs = OrderedSet(
            [
                key
                for key in self.local_output_dependency_map[given_conn][1]
                if key in self.conns.input_keys
            ]
        )
        # Make search from intermediate keys to the input keys.
        key_stack = OrderedSet(
            [
                spec
                for spec in self.local_output_dependency_map[given_conn][1]
                if spec not in specs
            ]
        )
        while key_stack:
            conn_data = key_stack.pop()
            if conn_data.key in self.conns.input_keys:
                specs.add(conn_data)
            # key_stack.update(self.dependency_map.get(key.key, OrderedSet()))
            # TODO: add test checking the while
            key_stack |= (
                self.local_output_dependency_map[conn_data][1]
                if conn_data in self.local_output_dependency_map
                else OrderedSet()
            )
        return specs

    # Check if cycle occured in graph
    def look_for_cyclic_connection(
        self, target_conn: ConnectionData, specs: OrderedSet[ConnectionData]
    ) -> bool:
        if target_conn in (conns := OrderedSet([conn for conn in specs])):
            return True
        else:
            for conn in conns:
                if conn in self.local_output_dependency_map:
                    return self.look_for_cyclic_connection(
                        target_conn, self.local_output_dependency_map[conn][1]
                    )
            return False

    def merge_global_connections(
        self, conn1: ConnectionData, conn2: ConnectionData
    ) -> None:
        conn1_global_out_dependency = self._global_output_dependency_map.get(conn1)
        conn2_global_out_dependency = self._global_output_dependency_map.pop(
            conn2, None
        )
        # If conn1 and conn2 cache exists, add all conn2 dependencies to conn1
        if (
            conn1_global_out_dependency is not None
            and conn2_global_out_dependency is not None
        ):
            for dependent_conn in conn2_global_out_dependency:
                conn1_global_out_dependency.add(dependent_conn)

                self._global_input_dependency_map[dependent_conn].remove(conn2)
                self._global_input_dependency_map[dependent_conn].add(conn1)
        # If conn1 is not, but conn2 cache exists, move all conn2 dependencies to conn1
        elif conn2_global_out_dependency is not None:
            self._global_output_dependency_map[conn1] = conn2_global_out_dependency

            for dependent_conn in conn2_global_out_dependency:
                self._global_input_dependency_map[dependent_conn].remove(conn2)
                self._global_input_dependency_map[dependent_conn].add(conn1)

        conn1_global_in_dependency = self._global_input_dependency_map.get(conn1)
        conn2_global_in_dependency = self._global_input_dependency_map.pop(conn2, None)
        # If conn1 and conn2 cache exists, add all conn2 dependencies to conn1
        if (
            conn1_global_in_dependency is not None
            and conn2_global_in_dependency is not None
        ):
            for dependent_conn in conn2_global_in_dependency:
                conn1_global_in_dependency.add(dependent_conn)

                self._global_output_dependency_map[dependent_conn].remove(conn2)
                self._global_output_dependency_map[dependent_conn].add(conn1)
        # If conn1 is not, but conn2 cache exists, move all conn2 dependencies to conn1
        elif conn2_global_in_dependency is not None:
            self._global_input_dependency_map[conn1] = conn2_global_in_dependency

            for dependent_conn in conn2_global_in_dependency:
                self._global_output_dependency_map[dependent_conn].remove(conn2)
                self._global_output_dependency_map[dependent_conn].add(conn1)

    def merge_global_caches(self, conn1: ConnectionData, conn2: ConnectionData) -> None:
        conn1_global_out_cache = self._global_output_dependency_map_cache.get(conn1)
        conn2_global_out_cache = self._global_output_dependency_map_cache.pop(
            conn2, None
        )

        # If conn1 and conn2 cache exists, add all conn2 dependencies to conn1
        if conn1_global_out_cache is not None and conn2_global_out_cache is not None:
            for dependent_conn in conn2_global_out_cache:
                conn1_global_out_cache.add(dependent_conn)

                self._global_input_dependency_map_cache[dependent_conn].remove(conn2)
                self._global_input_dependency_map_cache[dependent_conn].add(conn1)

        # If conn1 is not, but conn2 cache exists, move all conn2 dependencies to conn1
        elif conn2_global_out_cache is not None:
            self._global_output_dependency_map_cache[conn1] = conn2_global_out_cache

            for dependent_conn in conn2_global_out_cache:
                self._global_input_dependency_map_cache[dependent_conn].remove(conn2)
                self._global_input_dependency_map_cache[dependent_conn].add(conn1)

        conn1_global_in_cache = self._global_input_dependency_map_cache.get(conn1)
        conn2_global_in_cache = self._global_input_dependency_map_cache.pop(conn2, None)

        # If conn1 and conn2 cache exists, add all conn2 dependencies to conn1
        if conn1_global_in_cache is not None and conn2_global_in_cache is not None:
            for dependent_conn in conn2_global_in_cache:
                conn1_global_in_cache.add(dependent_conn)

                self._global_output_dependency_map_cache[dependent_conn].remove(conn2)
                self._global_output_dependency_map_cache[dependent_conn].add(conn1)

        # If conn1 is not, but conn2 cache exists, move all conn2 dependencies to conn1
        elif conn2_global_in_cache is not None:
            self._global_input_dependency_map_cache[conn1] = conn2_global_in_cache

            for dependent_conn in conn2_global_in_cache:
                self._global_output_dependency_map_cache[dependent_conn].remove(conn2)
                self._global_output_dependency_map_cache[dependent_conn].add(conn1)
