# Copyright 2022 Synnada, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import platform
from collections.abc import Mapping

import numpy as np
import pytest
import torch

import mithril as ml
from examples.flux.auto_encoder import (
    attn_block,
    decoder,
    downsample,
    encoder,
    resnet_block,
    upsample,
)
from examples.flux.layers import apply_rope as apply_rope_mithril
from examples.flux.layers import attention as attention_mithril
from examples.flux.layers import (
    double_stream_block,
    embed_nd,
    last_layer,
    modulation,
    qk_norm,
    rms_norm,
    rope,
    single_stream_block,
)
from examples.flux.layers import mlp_embedder as mlp_embedder_mithril
from examples.flux.layers import timestep_embedding as timestep_embedding_mithril

from .flux_torch import (
    AttnBlock,
    Decoder,
    DoubleStreamBlock,
    Downsample,
    EmbedND,
    Encoder,
    LastLayer,
    MLPEmbedder,
    Modulation,
    QKNorm,
    ResnetBlock,
    RMSNorm,
    SingleStreamBlock,
    Upsample,
    apply_rope,
    attention,
    timestep_embedding,
)
from .flux_torch import rope as torch_rope

installed_backends: list[type[ml.Backend]] = [
    ml.TorchBackend,
    ml.JaxBackend,
    # ml.NumpyBackend, #slow
]

if platform.system() == "Darwin":
    installed_backends.append(ml.MlxBackend)


class TestLayers:
    @pytest.mark.parametrize("backend_type", installed_backends)
    def test_resnet_block(self, backend_type: type[ml.Backend]):
        input_shape = [2, 64, 32, 48]
        m_model = resnet_block(64, 64)
        o_model = ResnetBlock(64, 64)

        backend = backend_type()

        pm = ml.compile(
            m_model,
            backend=backend,
            shapes={"input": input_shape},
            data_keys={"input"},
            use_short_namings=False,
        )

        params = load_weights(pm.shapes, o_model, backend)

        torch_inp = torch.randn(input_shape)
        expected_result = o_model(torch_inp)

        inp = backend.array(torch_inp.numpy())
        res = pm.evaluate(params, {"input": inp})["output"]
        np.testing.assert_allclose(res, expected_result.detach(), 1e-5, 1e-5)  # type: ignore

    @pytest.mark.parametrize("backend_type", installed_backends)
    def test_attn_block(self, backend_type: type[ml.Backend]):
        m_model = attn_block(512)
        o_model = AttnBlock(512)

        backend = backend_type()

        pm = ml.compile(
            m_model,
            backend=backend,
            shapes={"input": [1, 512, 32, 32]},
            data_keys={"input"},
            use_short_namings=False,
        )

        params = load_weights(pm.shapes, o_model, backend)

        torch_inp = torch.randn([1, 512, 32, 32])
        expected_result = o_model(torch_inp)

        inp = backend.array(torch_inp.numpy())
        res = pm.evaluate(params, {"input": inp})["output"]
        np.testing.assert_allclose(res, expected_result.detach(), 1e-5, 1e-5)  # type: ignore

    @pytest.mark.parametrize("backend_type", installed_backends)
    def test_downsample(self, backend_type: type[ml.Backend]):
        m_model = downsample(512)
        o_model = Downsample(512)

        backend = backend_type()

        pm = ml.compile(
            m_model,
            backend=backend,
            shapes={"input": [2, 512, 32, 48]},
            data_keys={"input"},
            use_short_namings=False,
        )

        params = load_weights(pm.shapes, o_model, backend)

        torch_inp = torch.randn([8, 512, 32, 48])
        expected_result = o_model(torch_inp)

        inp = backend.array(torch_inp.numpy())
        res = pm.evaluate(params, {"input": inp})["output"]
        np.testing.assert_allclose(res, expected_result.detach(), 1e-5, 1e-5)  # type: ignore

    @pytest.mark.parametrize("backend_type", installed_backends)
    def test_upsample(self, backend_type: type[ml.Backend]):
        m_model = upsample(64)
        o_model = Upsample(64)

        backend = backend_type()

        pm = ml.compile(
            m_model,
            backend=backend,
            shapes={"input": [1, 64, 32, 48]},
            data_keys={"input"},
            use_short_namings=False,
        )

        params = load_weights(pm.shapes, o_model, backend)

        torch_inp = torch.randn([1, 64, 32, 48])
        expected_result = o_model(torch_inp)

        inp = backend.array(torch_inp.numpy())
        res = pm.evaluate(params, {"input": inp})["output"]
        np.testing.assert_allclose(res, expected_result.detach(), 1e-5, 1e-5)  # type: ignore

    @pytest.mark.parametrize("backend_type", installed_backends)
    def test_encoder(self, backend_type: type[ml.Backend]):
        params: dict[str, int | list[int]] = {
            "resolution": 256,
            "in_channels": 3,
            "ch": 128,
            "ch_mult": [1, 2, 4, 4],
            "num_res_blocks": 2,
            "z_channels": 16,
        }
        m_model = encoder(**params)  # type: ignore
        o_model = Encoder(**params)  # type: ignore

        backend = backend_type()

        pm = ml.compile(
            m_model,
            backend=backend,
            shapes={"input": [1, 3, 256, 256]},
            data_keys={"input"},
            use_short_namings=False,
        )

        params = load_weights(pm.shapes, o_model, backend)

        torch_inp = torch.randn([1, 3, 256, 256])
        expected_result = o_model(torch_inp)

        inp = backend.array(torch_inp.numpy())
        res = pm.evaluate(params, {"input": inp})["output"]
        np.testing.assert_allclose(res, expected_result.detach(), 5e-5, 5e-5)  # type: ignore

    @pytest.mark.parametrize("backend_type", installed_backends)
    def test_decoder(self, backend_type: type[ml.Backend]):
        params: dict[str, int | list[int]] = {
            "ch": 128,
            "out_ch": 3,
            "ch_mult": [1, 2, 4, 4],
            "num_res_blocks": 2,
            "in_channels": 3,
            "resolution": 256,
            "z_channels": 16,
        }
        m_model = decoder(**params)  # type: ignore
        o_model = Decoder(**params)  # type: ignore

        backend = backend_type()

        pm = ml.compile(
            m_model,
            backend=backend,
            shapes={"input": [1, 16, 32, 32]},
            data_keys={"input"},
            jit=False,
            use_short_namings=False,
        )

        params = load_weights(pm.shapes, o_model, backend)

        torch_inp = torch.randn([1, 16, 32, 32])
        expected_result = o_model(torch_inp)

        inp = backend.array(torch_inp.numpy())
        res = pm.evaluate(params, {"input": inp})["output"]
        np.testing.assert_allclose(res, expected_result.detach(), 1e-4, 1e-4)  # type: ignore

    @pytest.mark.parametrize("backend_type", installed_backends)
    def test_rms_norm(self, backend_type: type[ml.Backend]):
        B, L, H, D = 1, 24, 4080, 128
        input_ref = torch.randn(B, L, H, D)
        o_model = RMSNorm(dim=128)

        backend = backend_type()

        pm = ml.compile(
            rms_norm(dim=128),
            backend=backend,
            shapes={"input": [B, L, H, D]},
            data_keys={"input"},
            use_short_namings=False,
        )

        params = load_weights(pm.shapes, o_model, backend)

        expected_result = o_model(input_ref)

        inp = backend.array(input_ref.numpy())
        res = pm.evaluate(params, {"input": inp})["output"]
        np.testing.assert_allclose(res, expected_result.detach(), 1e-6, 1e-6)  # type: ignore

    @pytest.mark.parametrize("backend_type", installed_backends)
    def test_qk_norm(self, backend_type: type[ml.Backend]):
        B, L, H, D = 1, 24, 4080, 128
        q_ref = torch.randn(B, L, H, D)
        k_ref = torch.randn(B, L, H, D)
        v_ref = torch.randn(B, L, H, D)
        o_model = QKNorm(dim=128)

        backend = backend_type()

        pm = ml.compile(
            qk_norm(128),
            backend=backend,
            shapes={"q_in": [B, L, H, D], "k_in": [B, L, H, D]},
            data_keys={"q_in", "k_in"},
            use_short_namings=False,
        )

        params = load_weights(pm.shapes, o_model, backend)

        q_out_ref, k_out_ref = o_model(q_ref, k_ref, v_ref)

        q_in = backend.array(q_ref.numpy())
        k_in = backend.array(k_ref.numpy())
        res = pm.evaluate(params, {"q_in": q_in, "k_in": k_in})

        np.testing.assert_allclose(res["q_out"], q_out_ref.detach(), 1e-6, 1e-6)  # type: ignore
        np.testing.assert_allclose(res["k_out"], k_out_ref.detach(), 1e-6, 1e-6)  # type: ignore

    @pytest.mark.parametrize("backend_type", installed_backends)
    def test_modulation(self, backend_type: type[ml.Backend]):
        H, W = 1, 3072
        input_ref = torch.randn(H, W)
        o_model = Modulation(dim=3072, double=False)

        backend = backend_type()

        pm = ml.compile(
            modulation(3072, False),
            backend=backend,
            shapes={"input": [H, W]},
            data_keys={"input"},
            use_short_namings=False,
        )

        params = load_weights(pm.shapes, o_model, backend)

        out = o_model(input_ref)

        inp = backend.array(input_ref.numpy())
        res = pm.evaluate(params, {"input": inp})

        np.testing.assert_allclose(res["mod_1"][0], out[0].shift.detach(), 1e-5, 1e-5)  # type: ignore
        np.testing.assert_allclose(res["mod_1"][1], out[0].scale.detach(), 1e-5, 1e-5)  # type: ignore
        np.testing.assert_allclose(res["mod_1"][2], out[0].gate.detach(), 1e-5, 1e-5)  # type: ignore

    @pytest.mark.parametrize("backend_type", installed_backends)
    def test_single_stream_block(self, backend_type: type[ml.Backend]):
        hidden_size = 3072
        num_heads = 24
        mlp_ratio = 4.0

        input_ref = torch.randn(1, 4336, 3072)
        vec_ref = torch.randn(1, 3072)
        pe_ref = torch.randn(1, 1, 4336, 64, 2, 2)
        o_model = SingleStreamBlock(hidden_size, num_heads, mlp_ratio)

        backend = backend_type()

        pm = ml.compile(
            single_stream_block(hidden_size, num_heads, mlp_ratio),
            backend=backend,
            shapes={
                "input": [1, 4336, 3072],
                "vec": [1, 3072],
                "pe": [1, 1, 4336, 64, 2, 2],
            },
            data_keys={"input", "vec", "pe"},
            use_short_namings=False,
        )

        params = load_weights(pm.shapes, o_model, backend)

        expected_result = o_model(input_ref, vec_ref, pe_ref)

        inp = backend.array(input_ref.numpy())
        vec = backend.array(vec_ref.numpy())
        pe = backend.array(pe_ref.numpy())
        res = pm.evaluate(params, {"input": inp, "vec": vec, "pe": pe})

        np.testing.assert_allclose(res["output"], expected_result.detach(), 1e-5, 1e-5)  # type: ignore

    @pytest.mark.parametrize("backend_type", installed_backends)
    def test_double_stream_block(self, backend_type: type[ml.Backend]):
        hidden_size = 3072
        num_heads = 24
        mlp_ratio = 4.0

        img_ref = torch.randn(1, 4080, 3072)
        txt_ref = torch.randn(1, 256, 3072)
        vec_ref = torch.randn(1, 3072)
        pe_ref = torch.randn(1, 1, 4336, 64, 2, 2)
        o_model = DoubleStreamBlock(hidden_size, num_heads, mlp_ratio)

        backend = backend_type()

        pm = ml.compile(
            double_stream_block(hidden_size, num_heads, mlp_ratio),
            backend=backend,
            shapes={
                "img": [1, 4080, 3072],
                "txt": [1, 256, 3072],
                "vec": [1, 3072],
                "pe": [1, 1, 4336, 64, 2, 2],
            },
            data_keys={"img", "txt", "vec", "pe"},
            use_short_namings=False,
        )

        params = load_weights(pm.shapes, o_model, backend)

        img_out_ref, txt_out_ref = o_model(img_ref, txt_ref, vec_ref, pe_ref)

        img = backend.array(img_ref.numpy())
        txt = backend.array(txt_ref.numpy())
        vec = backend.array(vec_ref.numpy())
        pe = backend.array(pe_ref.numpy())
        res = pm.evaluate(params, {"img": img, "txt": txt, "vec": vec, "pe": pe})

        np.testing.assert_allclose(res["img_out"], img_out_ref.detach(), 1e-5, 1e-5)  # type: ignore
        np.testing.assert_allclose(res["txt_out"], txt_out_ref.detach(), 1e-5, 1e-5)  # type: ignore

    @pytest.mark.parametrize("backend_type", installed_backends)
    def test_apply_rope(self, backend_type: type[ml.Backend]):
        B, L, H, D = 1, 24, 4336, 128
        q_ref = torch.randn(B, L, H, D)
        k_ref = torch.randn(B, L, H, D)
        pe_ref = torch.randn(1, 1, H, D // 2, 2, 2)

        backend = backend_type()

        pm = ml.compile(
            apply_rope_mithril(),
            backend=backend,
            shapes={
                "xq": [B, L, H, D],
                "xk": [B, L, H, D],
                "freqs_cis": [1, 1, H, D // 2, 2, 2],
            },
            data_keys={"xq", "xk", "freqs_cis"},
            use_short_namings=False,
            inference=True,
        )

        expected_res = apply_rope(q_ref, k_ref, pe_ref)

        q = backend.array(q_ref.numpy())
        k = backend.array(k_ref.numpy())
        pe = backend.array(pe_ref.numpy())
        res = pm.evaluate({}, {"xq": q, "xk": k, "freqs_cis": pe})

        np.testing.assert_allclose(res["xq_out"], expected_res[0].detach(), 1e-6, 1e-6)  # type: ignore
        np.testing.assert_allclose(res["xk_out"], expected_res[1].detach(), 1e-6, 1e-6)  # type: ignore

    @pytest.mark.parametrize("backend_type", installed_backends)
    def test_time_embeddings(self, backend_type: type[ml.Backend]):
        input_ref = torch.ones([1])
        expected_res = timestep_embedding(input_ref, 256)

        backend = backend_type()

        pm = ml.compile(
            timestep_embedding_mithril(256),
            backend=backend,
            shapes={"input": [1]},
            data_keys={"input"},
            use_short_namings=False,
            inference=True,
        )

        input = backend.array(input_ref.numpy())
        res = pm.evaluate({}, {"input": input})

        np.testing.assert_allclose(res["output"], expected_res, 1e-4, 1e-4)  # type: ignore

    @pytest.mark.parametrize("backend_type", installed_backends)
    def test_mlp_embedder(self, backend_type: type[ml.Backend]):
        m_model = mlp_embedder_mithril(64)
        o_model = MLPEmbedder(48, 64)

        backend = backend_type()

        pm = ml.compile(
            m_model,
            backend=backend,
            shapes={"input": [1, 64, 32, 48]},
            data_keys={"input"},
            use_short_namings=False,
        )

        params = load_weights(pm.shapes, o_model, backend)

        torch_inp = torch.randn([1, 64, 32, 48])
        expected_result = o_model(torch_inp)

        inp = backend.array(torch_inp.numpy())
        res = pm.evaluate(params, {"input": inp})["output"]
        np.testing.assert_allclose(res, expected_result.detach(), 1e-5, 1e-5)  # type: ignore

    @pytest.mark.parametrize("backend_type", installed_backends)
    def test_last_layer(self, backend_type: type[ml.Backend]):
        hidden_size = 256
        out_channels = 3
        patch_size = 2

        input_ref = torch.randn(1, 512, 256)
        vec_ref = torch.randn(1, 256)
        o_model = LastLayer(hidden_size, patch_size, out_channels)

        backend = backend_type()

        pm = ml.compile(
            last_layer(hidden_size, patch_size, out_channels),
            backend=backend,
            shapes={"input": [1, 512, 256], "vec": [1, 256]},
            data_keys={"input", "vec"},
            use_short_namings=False,
        )

        params = load_weights(pm.shapes, o_model, backend)

        expected_result = o_model(input_ref, vec_ref)

        input = backend.array(input_ref.numpy())
        vec = backend.array(vec_ref.numpy())
        res = pm.evaluate(params, {"input": input, "vec": vec})

        np.testing.assert_allclose(res["output"], expected_result.detach(), 1e-5, 1e-5)  # type: ignore

    @pytest.mark.parametrize("backend_type", installed_backends)
    def test_rope(self, backend_type: type[ml.Backend]):
        dim = 256
        theta = 4
        pos_ref = torch.rand(dim, 2)

        backend = backend_type()

        pm = ml.compile(
            rope(dim, theta),
            backend=backend,
            shapes={"input": [dim, 2]},
            data_keys={"input"},
            use_short_namings=False,
            jit=False,
            inference=True,
        )

        expected_result = torch_rope(pos_ref, dim, theta)

        pos = backend.array(pos_ref.numpy())
        res = pm.evaluate({}, {"input": pos})

        np.testing.assert_allclose(res["output"], expected_result, rtol=1e-5, atol=1e-5)  # type: ignore

    @pytest.mark.parametrize("backend_type", installed_backends)
    def test_embednd(self, backend_type: type[ml.Backend]):
        dim = 128
        theta = 10_000
        axes_dim = [16, 56, 56]
        input_ref = torch.rand(1, 4336, 3)
        o_model = EmbedND(dim=dim, theta=theta, axes_dim=axes_dim)

        backend = backend_type()

        pm = ml.compile(
            embed_nd(theta=theta, axes_dim=axes_dim),
            backend=backend,
            shapes={"input": [1, 4336, 3]},
            data_keys={"input"},
            use_short_namings=False,
            inference=True,
        )

        expected_result = o_model(input_ref)

        input = backend.array(input_ref.numpy())
        res = pm.evaluate({}, {"input": input})

        np.testing.assert_allclose(res["output"], expected_result, rtol=1e-5, atol=1e-5)  # type: ignore

    @pytest.mark.parametrize("backend_type", installed_backends)
    def test_attention(self, backend_type: type[ml.Backend]):
        B, L, H, D = 1, 24, 4336, 128
        q_ref = torch.randn(B, L, H, D)
        k_ref = torch.randn(B, L, H, D)
        v_ref = torch.randn(B, L, H, D)
        pe_ref = torch.randn(1, 1, H, D // 2, 2, 2)

        backend = backend_type()

        pm = ml.compile(
            attention_mithril(),
            backend=backend,
            shapes={
                "q": [B, L, H, D],
                "k": [B, L, H, D],
                "v": [B, L, H, D],
                "pe": [1, 1, H, D // 2, 2, 2],
            },
            data_keys={"q", "k", "v", "pe"},
            use_short_namings=False,
            inference=True,
        )

        expected_res = attention(q_ref, k_ref, v_ref, pe_ref)

        q = backend.array(q_ref.numpy())
        k = backend.array(k_ref.numpy())
        v = backend.array(v_ref.numpy())
        pe = backend.array(pe_ref.numpy())
        res = pm.evaluate({}, {"q": q, "k": k, "v": v, "pe": pe})

        np.testing.assert_allclose(res["output"], expected_res.detach(), 1e-5, 1e-5)  # type: ignore


## Utils


def load_weights(
    param_shapes: Mapping, torch_model: torch.nn.Module, backend: ml.Backend
):
    ml_params = {}
    torch_state_dict = torch_model.state_dict()

    for torch_key in torch_state_dict:
        ml_key = torch_key.replace(".", "_").lower()
        if ml_key not in param_shapes:
            continue

        param_shape = param_shapes[ml_key]

        if torch_state_dict[torch_key].shape != param_shape:
            parameter = torch_state_dict[torch_key].numpy().reshape(param_shape)
        else:
            parameter = torch_state_dict[torch_key].numpy().reshape(param_shape)
        ml_params[ml_key] = backend.array(parameter)

    return ml_params
